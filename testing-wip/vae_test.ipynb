{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPi3zOPJtIza"
      },
      "source": [
        "**Apogee Data Simple VAE Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kt-ZMAZGmU1u"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-56c4a4a21e87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Basic imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# Basic imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "# from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "from astropy.io import fits\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KNbKYArtDJN"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSJ0N0cTo2yA"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "dataset_path = '~/datasets'\n",
        "cuda = True\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "# Data parameters\n",
        "input_rows = 1000   # Number of spectra to input\n",
        "batch_size = 20\n",
        "validation_split = .2   # Fraction of dataset to reserve for test\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "shuffle_toggle = True\n",
        "\n",
        "# Model Dimensions\n",
        "x_dim  = 7514\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "\n",
        "# Learning rate\n",
        "# Default 0.001\n",
        "lr = 0.01\n",
        "\n",
        "# Num epochs\n",
        "epochs = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Y2UrIArFBs"
      },
      "source": [
        "**Process and Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTbI8VPmxIXi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WljSeTcCxmus"
      },
      "outputs": [],
      "source": [
        "def starInfoDebug():\n",
        "  for i in range(10):\n",
        "      # Title information\n",
        "      print(\"\\n**************** Looking at index \", i, \" ****************\")\n",
        "      print(\"APSTAR ID: \", star['apstar_id'][i],\\\n",
        "          \"\\nTARGET_ID: \", star['target_id'][i],\\\n",
        "              \"\\nASPCAP_ID: \", star['aspcap_id'][i])\n",
        "      \n",
        "      print(\"\\nBasic Stats:\")\n",
        "      print(\"SNR: \", star['snr'][i])\n",
        "      print(\"Effective Temp (K) \\t ASPCAP: \", star['teff_spec'][i], 'AstroNN:', star_astroNN['TEFF'][i])\n",
        "      print(\"Surface G (log(cm/s^2) \\t ASPCAP: \", star['logg_spec'][i], 'AstroNN:', star_astroNN['LOGG'][i])\n",
        "\n",
        "      #j = ind[i]\n",
        "      j = i\n",
        "\n",
        "      plt.subplot(5, 2, j+1)\n",
        "      plt.title(star['aspcap_id'][i])\n",
        "      plt.xlabel('Wavelength')\n",
        "      plt.ylabel('Relative Flux')\n",
        "      # 7514 data points for each spectra\n",
        "      plt.plot(star_spectra[j])\n",
        "\n",
        "      spectra_df = pd.DataFrame(star_spectra[j])\n",
        "      # Actual input data\n",
        "      print(\"\\nInput dataframe:\")\n",
        "      print(spectra_df)\n",
        "\n",
        "      #print(len(star_spectra[j]))\n",
        "      #plt.legend([star['aspcap_id'][i]])\n",
        "      #plt.show()\n",
        "\n",
        "      # Abundances, other info\n",
        "      print(\"\\nAbundances, additional info (ASPCAP):\")\n",
        "      print(star['ra'][j], star['dec'][j], star['glon'][j], star['glat'][j],\\\n",
        "          star['vhelio_avg'][j], star['vscatter'][j],\\\n",
        "          star['teff'][j], star['teff_err'][j],\\\n",
        "          star['logg'][j], star['logg_err'][j],\\\n",
        "          star['m_h'][j], star['m_h_err'][j],\\\n",
        "          star['alpha_m'][j], star['alpha_m_err'][j],\\\n",
        "          star['c_fe'][j], star['c_fe_err'][j],\\\n",
        "          star['cI_fe'][j], star['cI_fe_err'][j],\\\n",
        "          star['n_fe'][j], star['n_fe_err'][j],\\\n",
        "          star['o_fe'][j], star['o_fe_err'][j],\\\n",
        "          star['na_fe'][j], star['na_fe_err'][j],\\\n",
        "          star['mg_fe'][j], star['mg_fe_err'][j],\\\n",
        "          star['al_fe'][j], star['al_fe_err'][j],\\\n",
        "          star['si_fe'][j], star['si_fe_err'][j],\\\n",
        "          star['p_fe'][j], star['p_fe_err'][j],\\\n",
        "          star['s_fe'][j], star['s_fe_err'][j],\\\n",
        "          star['k_fe'][j], star['k_fe_err'][j],\\\n",
        "          star['ca_fe'][j], star['ca_fe_err'][j],\\\n",
        "          star['ti_fe'][j], star['ti_fe_err'][j],\\\n",
        "          star['v_fe'][j], star['v_fe_err'][j],\\\n",
        "          star['cr_fe'][j], star['cr_fe_err'][j],\\\n",
        "          star['mn_fe'][j], star['mn_fe_err'][j],\\\n",
        "          star['fe_h'][j], star['fe_h_err'][j],\\\n",
        "          star['co_fe'][j], star['co_fe_err'][j],\\\n",
        "          star['ni_fe'][j], star['ni_fe_err'][j],\\\n",
        "          star['cu_fe'][j], star['cu_fe_err'][j],\\\n",
        "          star['ge_fe'][j], star['ge_fe_err'][j],\\\n",
        "          star['rb_fe'][j], star['rb_fe_err'][j],\\\n",
        "          star['aspcapflags'][j], star['starflags'][j])\n",
        "\n",
        "      plt.subplots_adjust(hspace=1)\n",
        "      #plt.tight_layout()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI_JwkURxMB0"
      },
      "outputs": [],
      "source": [
        "print(\"\\n ********************** Opening FITS files from drive **********************\")\n",
        "\n",
        "star_hdus = fits.open('/content/drive/MyDrive/2021-2022 Engsci Thesis/allStar-r12-l33.fits')\n",
        "astroNN_hdus = fits.open('/content/drive/MyDrive/2021-2022 Engsci Thesis/apogee_astroNN-DR16-v1.fits')\n",
        "star_spec = fits.open('/content/drive/MyDrive/2021-2022 Engsci Thesis/contspec_dr16_final.fits')\n",
        "\n",
        "star = star_hdus[1].data\n",
        "star_astroNN = astroNN_hdus[1].data\n",
        "star_spectra = star_spec[0].data\n",
        "\n",
        "star_hdus.close()\n",
        "astroNN_hdus.close()\n",
        "star_spec.close()\n",
        "\n",
        "print(\"Number of spectra: \", len(star))\n",
        "print(\"Data points per spectra: \", len(star_spectra[1]))\n",
        "\n",
        "# starInfoDebug()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDGazuZYICa1"
      },
      "source": [
        "**Dataset Class for Spectral data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI2XwUlO5wqQ"
      },
      "outputs": [],
      "source": [
        "# https://visualstudiomagazine.com/articles/2020/09/10/pytorch-dataloader.aspx\n",
        "\n",
        "class spectraDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  # Num rows = max number of spectra to load\n",
        "  def __init__(self, src, num_rows=None):\n",
        "    if num_rows == None:\n",
        "      spectra = src.astype(np.float32)\n",
        "    else:\n",
        "      spectra = src.astype(np.float32)[0:num_rows]\n",
        "\n",
        "    # y_tmp = np.loadtxt(src_file, max_rows=num_rows,\n",
        "    #   usecols=7, delimiter=\"\\t\", skiprows=0,\n",
        "    #   dtype=np.long)\n",
        "\n",
        "    self.x_data = torch.tensor(spectra, dtype=torch.float32)\n",
        "\n",
        "    # self.y_data = T.tensor(y_tmp,\n",
        "    #   dtype=T.long).to(DEVICE)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)  # required\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # if T.is_tensor(idx):\n",
        "    #   idx = idx.tolist()\n",
        "    # preds = self.x_data[idx, 0:7]\n",
        "    # pol = self.y_data[idx]\n",
        "    # sample = \\\n",
        "    #   { 'predictors' : preds, 'political' : pol }\n",
        "\n",
        "    sample = self.x_data[idx]\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOkDqqAoH_ou"
      },
      "source": [
        "**Train/Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGYcFC8BKLcD"
      },
      "outputs": [],
      "source": [
        "# # Reduce the dataset down to a manageable size, based on input_rows hyperparameter\n",
        "np.random.seed(42)\n",
        "random_reduced_idx = list(np.random.choice(len(star_spectra), input_rows, replace=False))\n",
        "\n",
        "# # Grab only spectra with indices randomly selected from above\n",
        "#reduced_star_spectra = np.take(star_spectra, random_reduced_idx, 0)\n",
        "reduced_star_spectra = star_spectra[random_reduced_idx]\n",
        "print(pd.DataFrame(reduced_star_spectra))\n",
        "\n",
        "\n",
        "#Basic subset testing\n",
        "#reduced_star_spectra = star_spectra[0:input_rows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfJHX-yFpdiK"
      },
      "outputs": [],
      "source": [
        "#Normalize (between 0 and 1)\n",
        "\n",
        "# sum_of_rows = reduced_star_spectra.sum(axis=1)\n",
        "# reduced_star_spectra = reduced_star_spectra / sum_of_rows[:, np.newaxis]\n",
        "\n",
        "# reduced_maximum = np.amax(reduced_star_spectra)\n",
        "# print(reduced_maximum)\n",
        "# reduced_star_spectra = reduced_star_spectra/reduced_maximum\n",
        "\n",
        "# Normalization array\n",
        "reduced_maximum = np.zeros(len(reduced_star_spectra))\n",
        "\n",
        "for i in range(0, len(reduced_star_spectra)):\n",
        "  reduced_maximum[i] = np.amax(reduced_star_spectra[i])\n",
        "  reduced_star_spectra[i] = reduced_star_spectra[i]/reduced_maximum[i]\n",
        "\n",
        "print(pd.DataFrame(reduced_star_spectra))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC-aEHs9pV4e"
      },
      "outputs": [],
      "source": [
        "# Final normalized, reduced inputs\n",
        "train_dataset = spectraDataset(reduced_star_spectra)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnK9JzFyH_O_"
      },
      "outputs": [],
      "source": [
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(reduced_star_spectra)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "\n",
        "print(\"Splitting dataset at\", split)\n",
        "\n",
        "# If shuffling is enabled, use random seed to shuffle data indices\n",
        "if shuffle_toggle:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "# Get training/validation indices\n",
        "train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Generate samplers\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKNkFIRY9Z7w"
      },
      "outputs": [],
      "source": [
        "# Generate data loaders\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, **kwargs)\n",
        "test_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=test_sampler, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmA4XGfG-e-Z"
      },
      "outputs": [],
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHy11ApKq3FM"
      },
      "outputs": [],
      "source": [
        "# mnist_transform = transforms.Compose([\n",
        "#         transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} \n",
        "\n",
        "# train_dataset_mnist = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
        "# test_dataset_mnist  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
        "\n",
        "\n",
        "# train_loader_mnist = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "# test_loader_mnist  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3aX2FK8rTHO"
      },
      "source": [
        "Implement Simple VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HONX58QMrSUl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    A simple implementation of Gaussian MLP Encoder and Decoder\n",
        "\"\"\"\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
        "    self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
        "    self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
        "    \n",
        "    self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "    \n",
        "    self.training = True\n",
        "      \n",
        "  def forward(self, x):\n",
        "    h_       = self.LeakyReLU(self.FC_input(x))\n",
        "    h_       = self.LeakyReLU(self.FC_input2(h_))\n",
        "    mean     = self.FC_mean(h_)\n",
        "    log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance \n",
        "                                                  #             (i.e., parameters of simple tractable normal distribution \"q\"\n",
        "    \n",
        "    return mean, log_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo-_3cU1rYqv"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "    self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    h = self.LeakyReLU(self.FC_hidden(x))\n",
        "    h = self.LeakyReLU(self.FC_hidden2(h))\n",
        "    \n",
        "    # originally torch.sigmoid, but output range incorrect\n",
        "    x_hat = torch.sigmoid(self.FC_output(h))\n",
        "    return x_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0hRtlO5ralJ"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, Encoder, Decoder):\n",
        "    super(Model, self).__init__()\n",
        "    self.Encoder = Encoder\n",
        "    self.Decoder = Decoder\n",
        "      \n",
        "  def reparameterization(self, mean, var):\n",
        "    epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
        "    z = mean + var*epsilon                          # reparameterization trick\n",
        "    return z\n",
        "      \n",
        "              \n",
        "  def forward(self, x):\n",
        "    # Generate mean, log var\n",
        "    mean, log_var = self.Encoder(x)\n",
        "\n",
        "    z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
        "    x_hat = self.Decoder(z)\n",
        "    \n",
        "    return x_hat, mean, log_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDqjx8Pyrekx"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg0P-A_LrhK5"
      },
      "outputs": [],
      "source": [
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "  reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "  KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "  return reproduction_loss + KLD\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYM5WBzTriny"
      },
      "outputs": [],
      "source": [
        "print(\"Start training VAE...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  overall_loss = 0\n",
        "  \n",
        "  for batch_idx, (x) in enumerate(train_loader):\n",
        "    x = x.view(batch_size, x_dim)\n",
        "    x = x.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    x_hat, mean, log_var = model(x)\n",
        "    loss = loss_function(x, x_hat, mean, log_var)\n",
        "    \n",
        "    overall_loss += loss.item()\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "      \n",
        "  print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size))\n",
        "    \n",
        "print(\"Finished!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GjN2krOr0nI"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (x) in enumerate(tqdm(test_loader)):\n",
        "    x = x.view(batch_size, x_dim)\n",
        "    x = x.to(DEVICE)\n",
        "    \n",
        "    x_hat, _, _ = model(x)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsJCDz_Ywk9C"
      },
      "outputs": [],
      "source": [
        "#x[1] = x[1] * reduced_maximum\n",
        "print(x[0])\n",
        "\n",
        "plt.title('Original Spectra')\n",
        "plt.xlabel('Wavelength')\n",
        "plt.ylabel('Relative Flux')\n",
        "# 7514 data points for each spectra\n",
        "plt.plot(x[0].cpu().numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcETlCUsOVOI"
      },
      "outputs": [],
      "source": [
        "#x_hat[1] = x_hat[1] * reduced_maximum\n",
        "\n",
        "print(x_hat[0])\n",
        "\n",
        "plt.title('Reconstructed Spectra')\n",
        "plt.xlabel('Wavelength')\n",
        "plt.ylabel('Relative Flux')\n",
        "# 7514 data points for each spectra\n",
        "plt.plot(x_hat[0].cpu().numpy())\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "vae_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
