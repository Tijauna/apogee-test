{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import corner\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dataset_path = '~/datasets'\n",
    "cuda = True\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# Data parameters\n",
    "input_rows = 10000   # Number of spectra to input, tried 20k before. Try 60k now.\n",
    "batch_size = 100    # Tested 100\n",
    "validation_split = .2   # Fraction of dataset to reserve for test\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "shuffle_toggle = False  # Redundant due to random reduced idx already implemented\n",
    "\n",
    "# Model Dimensions\n",
    "x_dim  = 7514\n",
    "# Originally 400, 200\n",
    "hidden_dim = 400\n",
    "latent_dim = 28\n",
    "\n",
    "# Learning rate\n",
    "# Default 0.001\n",
    "lr = 0.001\n",
    "# Gradient clipping\n",
    "clipping_value = 1\n",
    "\n",
    "# VAE Beta\n",
    "beta = 1\n",
    "# VAE Separability Loss scaling factor\n",
    "gamma = 1\n",
    "\n",
    "# Num epochs\n",
    "epochs = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    A simple implementation of Gaussian MLP Encoder and Decoder\n",
    "\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    # FC Linear version\n",
    "    self.FC_input = nn.Linear(input_dim+3, 3757)\n",
    "    # Try batch normalization\n",
    "    # nn.BatchNorm1d(hidden_dim)\n",
    "    self.FC_input2 = nn.Linear(3757, 1878)\n",
    "    self.FC_input3 = nn.Linear(1878, 939)\n",
    "    self.FC_input4 = nn.Linear(939, 469)\n",
    "    self.FC_input5 = nn.Linear(469, 234)\n",
    "    self.FC_input6 = nn.Linear(234, 117)\n",
    "    self.FC_input7 = nn.Linear(117, 58)\n",
    "    # self.FC_input8 = nn.Linear(58, 29)\n",
    "\n",
    "    # Increase number of layers!!\n",
    "\n",
    "    # Mean and log variance\n",
    "    self.FC_mean  = nn.Linear(58, latent_dim)\n",
    "    self.FC_var   = nn.Linear(58, latent_dim)\n",
    "    \n",
    "    self.LeakyReLU = nn.LeakyReLU()\n",
    "    self.gelu = torch.nn.GELU()\n",
    "    \n",
    "    self.training = True\n",
    "      \n",
    "  def forward(self, x):\n",
    "    h_ = self.gelu(self.FC_input(x))\n",
    "    h_ = self.gelu(self.FC_input2(h_))\n",
    "    h_ = self.gelu(self.FC_input3(h_))\n",
    "    h_ = self.gelu(self.FC_input4(h_))\n",
    "    h_ = self.gelu(self.FC_input5(h_))\n",
    "    h_ = self.gelu(self.FC_input6(h_))\n",
    "    h_ = self.gelu(self.FC_input7(h_))\n",
    "    # h_ = self.LeakyReLU(self.FC_input8(h_))\n",
    "\n",
    "    mean = self.FC_mean(h_)\n",
    "    log_var = self.FC_var(h_)                     # encoder produces mean and log of variance \n",
    "                                                  #             (i.e., parameters of simple tractable normal distribution \"q\"\n",
    "\n",
    "    # Last 3 columns of x should be TEFF, LOGG, FE_H\n",
    "    passed_parameters = x[:, -3:]\n",
    "    \n",
    "    return mean, log_var, passed_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    # FC Linear version\n",
    "    # self.FC_hidden = nn.Linear(latent_dim, 29)\n",
    "    self.FC_hidden2 = nn.Linear(latent_dim+3, 58)\n",
    "    self.FC_hidden3 = nn.Linear(58, 117)\n",
    "    self.FC_hidden4 = nn.Linear(117, 234)\n",
    "    self.FC_hidden5 = nn.Linear(234, 469)\n",
    "    self.FC_hidden6 = nn.Linear(469, 939)\n",
    "    self.FC_hidden7 = nn.Linear(939, 1878)\n",
    "    self.FC_hidden8 = nn.Linear(1878, 3757)\n",
    "    self.FC_output = nn.Linear(3757, output_dim)\n",
    "    \n",
    "    self.LeakyReLU = nn.LeakyReLU()\n",
    "    self.gelu = torch.nn.GELU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # h = self.LeakyReLU(self.FC_hidden(x))\n",
    "    h = self.gelu(self.FC_hidden2(x))\n",
    "    h = self.gelu(self.FC_hidden3(h))\n",
    "    h = self.gelu(self.FC_hidden4(h))\n",
    "    h = self.gelu(self.FC_hidden5(h))\n",
    "    h = self.gelu(self.FC_hidden6(h))\n",
    "    h = self.gelu(self.FC_hidden7(h))\n",
    "    h = self.gelu(self.FC_hidden8(h))\n",
    "  \n",
    "    # originally torch.sigmoid, but output range incorrect\n",
    "    # Replace with smooth function - look into this!!\n",
    "    # Softplus, gelu \n",
    "\n",
    "    #activation = torch.nn.Softplus()\n",
    "    activation = torch.nn.GELU()\n",
    "    x_hat = activation(self.FC_output(h))\n",
    "    #x_hat = torch.nn.GELU(self.FC_output(h))\n",
    "    #print(x_hat)\n",
    "    return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self, Encoder, Decoder):\n",
    "    super(Model, self).__init__()\n",
    "    self.Encoder = Encoder\n",
    "    self.Decoder = Decoder\n",
    "\n",
    "  def reparameterization(self, mean, var):\n",
    "    epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "    z = mean + var*epsilon                          # reparameterization trick\n",
    "    return z\n",
    "  \n",
    "  # Modified to explicitly pass reduced_parameter_errors\n",
    "  def forward(self, x, originalIndex, batch_size, reduced_parameter_errors):\n",
    "    # Generate mean, log var\n",
    "    mean, log_var, passed_parameters = self.Encoder(x)\n",
    "\n",
    "    z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "\n",
    "    passed_parameters_errors = torch.from_numpy(reduced_parameter_errors[originalIndex : originalIndex + batch_size].astype(np.float32)).to(DEVICE)\n",
    "    passed_parameters_errors = torch.nn.functional.relu(passed_parameters_errors)\n",
    "    u = self.reparameterization(passed_parameters, passed_parameters_errors)\n",
    "    z = torch.hstack((z, u))\n",
    "    x_hat = self.Decoder(z)\n",
    "    \n",
    "    # Modified in evaluation file to return latent vector z\n",
    "    return x_hat, mean, log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('outputs/3-18-2022-kld-boost/vae_best_300_2022-03-21.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ********************** Opening FITS files from drive **********************\")\n",
    "\n",
    "star_hdus = fits.open('allStar-r12-l33.fits')\n",
    "astroNN_hdus = fits.open('apogee_astroNN-DR16-v1.fits')\n",
    "star_spec = fits.open('contspec_dr16_final.fits')\n",
    "\n",
    "star = star_hdus[1].data\n",
    "star_astroNN = astroNN_hdus[1].data\n",
    "star_spectra = star_spec[0].data\n",
    "\n",
    "star_hdus.close()\n",
    "astroNN_hdus.close()\n",
    "star_spec.close()\n",
    "\n",
    "print(\"Number of spectra: \", len(star))\n",
    "print(\"Data points per spectra: \", len(star_spectra[1]))\n",
    "\n",
    "# starInfoDebug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_mask = fits.open('contspec_dr16_mask.fits')\n",
    "# star_err = fits.open('contspec_dr16_err.fits')\n",
    "\n",
    "# star_mask_data = star_mask[0].data\n",
    "# star_err_data = star_err[0].data\n",
    "\n",
    "# star_mask.close()\n",
    "# star_err.close()\n",
    "\n",
    "# print(\"Number of masks: \", len(star_mask_data))\n",
    "# print(\"Data points per maskk: \", len(star_mask_data[1]))\n",
    "\n",
    "# print(\"Number of errors: \", len(star_err_data))\n",
    "# print(\"Data points per error: \", len(star_err_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Star Analysis\n",
    "\n",
    "from astropy.table import Table\n",
    "dat = Table.read('allStar-r12-l33.fits', format='fits')\n",
    "names = [name for name in dat.colnames if len(dat[name].shape) <= 1]\n",
    "df = dat[names].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://visualstudiomagazine.com/articles/2020/09/10/pytorch-dataloader.aspx\n",
    "\n",
    "class spectraDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  # Num rows = max number of spectra to load\n",
    "  def __init__(self, src, num_rows=None):\n",
    "    if num_rows == None:\n",
    "      spectra = src.astype(np.float32)\n",
    "    else:\n",
    "      spectra = src.astype(np.float32)[0:num_rows]\n",
    "\n",
    "    # y_tmp = np.loadtxt(src_file, max_rows=num_rows,\n",
    "    #   usecols=7, delimiter=\"\\t\", skiprows=0,\n",
    "    #   dtype=np.long)\n",
    "\n",
    "    self.x_data = torch.tensor(spectra, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    # self.y_data = T.tensor(y_tmp,\n",
    "    #   dtype=T.long).to(DEVICE)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)  # required\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # if T.is_tensor(idx):\n",
    "    #   idx = idx.tolist()\n",
    "    # preds = self.x_data[idx, 0:7]\n",
    "    # pol = self.y_data[idx]\n",
    "    # sample = \\\n",
    "    #   { 'predictors' : preds, 'political' : pol }\n",
    "\n",
    "    sample = self.x_data[idx]\n",
    "    # Modified March 21 2022 to return batch index\n",
    "    return sample, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(df, star_spectra, teff_min, teff_max, logg_min, logg_max, snr_min, snr_max):\n",
    "    # Surpress write on copy warning\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Isolate critical columns\n",
    "    star_df = df[['APSTAR_ID', 'TEFF_SPEC', 'LOGG_SPEC', 'SNR', 'ASPCAPFLAGS', 'STARFLAGS', 'FE_H']]\n",
    "\n",
    "    #We only include stars with no bad star flags set, SNR > 200, 4000 < teff < 5500, and logg < 3.5.\n",
    "    star_df_best = star_df.loc[(star_df['TEFF_SPEC'] < teff_max) & (star_df['TEFF_SPEC'] > teff_min) & \n",
    "    (star_df['LOGG_SPEC'] < logg_max) & (star_df['LOGG_SPEC'] > logg_min) & \n",
    "    (star_df['SNR'] < snr_max) & (star_df['SNR'] > snr_min)]\n",
    "\n",
    "    # Decode byte flags into strings\n",
    "    star_df_best['ASPCAPFLAGS'] = star_df_best['ASPCAPFLAGS'].str.decode(\"utf-8\")\n",
    "\n",
    "    # Strip out stars with STAR_BAD flag\n",
    "    star_df_best = star_df_best.loc[~(star_df_best['ASPCAPFLAGS'].str.contains(\"STAR_BAD\"))]\n",
    "\n",
    "    # Modified to add 3 parameters in 'u'\n",
    "    # Extract columns with three target parameters\n",
    "    star_df_best_parameters = star_df_best[[\"TEFF_SPEC\", \"LOGG_SPEC\", \"FE_H\"]]\n",
    "    star_df_best_parameters.reset_index()\n",
    "\n",
    "    # Make a new array with the associated errors for TEFF, LOGG, FE_H\n",
    "    # Have to transform 1d numpy array to 2d array for hstack. Transpose to get row -> Column\n",
    "    parameter_errors = np.hstack((star['teff_err'][None, :].T,star['logg_err'][None, :].T, star['fe_h_err'][None, :].T))\n",
    "    star_df_best_parameters_np = star_df_best_parameters.to_numpy()\n",
    "    star_df_best_parameters_np_std_dev = star_df_best_parameters_np.std(axis=0)\n",
    "    star_df_best_parameters_np -= star_df_best_parameters_np.mean(axis=0)\n",
    "    star_df_best_parameters_np /= star_df_best_parameters_np.std(axis=0)\n",
    "\n",
    "    #print(star_df_best.head(1))\n",
    "\n",
    "    # Update the star_spectra dataframe with only 'good' indices\n",
    "    star_spectra_filtered = star_spectra[star_df_best.index]\n",
    "    star_spectra_filtered = np.hstack((star_spectra_filtered, star_df_best_parameters_np))\n",
    "\n",
    "    ### Modified March 21 2022\n",
    "    parameter_errors = parameter_errors[star_df_best.index]\n",
    "    # Divide parameter errors by std dev of data\n",
    "    parameter_errors /= star_df_best_parameters_np_std_dev\n",
    "    \n",
    "    # Update masks, errors as well  \n",
    "    # star_err_data = star_err_data[star_df_best.index]\n",
    "    # star_mask_data = star_mask_data[star_df_best.index]\n",
    "\n",
    "    print(\"After applying data filters \" + str(len(star_spectra_filtered)) + \" spectra remaining\")\n",
    "\n",
    "    # # Reduce the dataset down to a manageable size, based on input_rows hyperparameter\n",
    "    np.random.seed(random_seed)\n",
    "    random_reduced_idx = list(np.random.choice(len(star_spectra_filtered), input_rows, replace=False))\n",
    "    \n",
    "    print(\"Reduced spectra count to \" + str(input_rows))\n",
    "\n",
    "    # # Grab only spectra with indices randomly selected from above\n",
    "    #reduced_star_spectra = np.take(star_spectra, random_reduced_idx, 0)\n",
    "    reduced_star_spectra = star_spectra_filtered[random_reduced_idx]\n",
    "\n",
    "    ### Modified March 21 2022\n",
    "    reduced_parameter_errors = parameter_errors[random_reduced_idx]\n",
    "\n",
    "    #print(pd.DataFrame(reduced_star_spectra))\n",
    "\n",
    "    # Normalize\n",
    "    for starRow in reduced_star_spectra:\n",
    "        starRow -= starRow.min()\n",
    "        #print(starRow.max())\n",
    "        if starRow.max() == 0:\n",
    "            #print(\"Found zero max\")\n",
    "            pass\n",
    "        else:\n",
    "            starRow /= starRow.max()\n",
    "\n",
    "    #print(pd.DataFrame(reduced_star_spectra))\n",
    "\n",
    "    # Final normalized, reduced inputs\n",
    "    train_dataset = spectraDataset(reduced_star_spectra)\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(reduced_star_spectra)\n",
    "    indices = list(range(dataset_size))\n",
    "    # split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "    # print(\"Splitting dataset at\", split)\n",
    "\n",
    "    # If shuffling is enabled, use random seed to shuffle data indices\n",
    "    if shuffle_toggle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    # Generate data loaders\n",
    "    kwargs = {'num_workers': 0}\n",
    "\n",
    "    # Try without random sampling (simple split on index)\n",
    "    # train_loader = DataLoader(train_dataset[split:], batch_size=batch_size, **kwargs)\n",
    "    test_loader = DataLoader(train_dataset, batch_size=batch_size, **kwargs)\n",
    "\n",
    "    # print('Batches in train:', len(train_loader))\n",
    "    print('Batches in test:', len(test_loader))\n",
    "\n",
    "    return test_loader, reduced_parameter_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High, Medium, Low SNR Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only include stars with no bad star flags set, SNR > 200, 4000 < teff < 5500, and logg < 3.5.\n",
    "\n",
    "low_snr_data, low_snr_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 0, snr_max = 100)\n",
    "\n",
    "medium_snr_data, medium_snr_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 100, snr_max = 200)\n",
    "\n",
    "high_snr_data, high_snr_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 200, snr_max = math.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High, Medium, Low Effective Temperature Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not enough samples when logg is restricted -> Need to verify\n",
    "\n",
    "low_teff_data, low_teff_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 0, \n",
    "teff_max = 4000, logg_min = 0, logg_max = 5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "medium_teff_data, medium_teff_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "high_teff_data, high_teff_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 5500, \n",
    "teff_max = math.inf, logg_min = 0, logg_max = 5, snr_min = 200, snr_max = math.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High, Medium, Low Surface Gravity Stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_logg_data, low_logg_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 2, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "medium_logg_data, medium_logg_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 2, logg_max = 3.5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "high_logg_data, high_logg_parameter_errors = fetch_data(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 3.5, logg_max = math.inf, snr_min = 200, snr_max = math.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Space Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.FC_mean.weight)\n",
    "print(encoder.FC_var.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.FC_mean.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed model helper function\n",
    "def feed_model(data_loader, parameter_errors):\n",
    "  latent_vector_array = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(data_loader)):\n",
    "      x = x.view(batch_size, x_dim+3)\n",
    "      x = x.to(DEVICE)\n",
    "\n",
    "      #print(\"Looking at original batch index:\", batch_idx)\n",
    "      x_hat, mean, log_var, z = model(x, batch_idx, batch_size, parameter_errors)\n",
    "\n",
    "      # Each batch has 100 latent vecs of size 28\n",
    "      for batchItem in z:\n",
    "        # Add to overall latent vector array\n",
    "        latent_vector_array += [batchItem.cpu().detach().numpy()]\n",
    "\n",
    "  return latent_vector_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed Model with Filtered Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_snr_array = feed_model(high_snr_data, high_snr_parameter_errors)\n",
    "medium_snr_array = feed_model(medium_snr_data, medium_snr_parameter_errors)\n",
    "low_snr_array = feed_model(low_snr_data, low_snr_parameter_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_teff_array = feed_model(high_teff_data, high_teff_parameter_errors)\n",
    "medium_teff_array = feed_model(medium_teff_data, medium_teff_parameter_errors)\n",
    "low_teff_array = feed_model(low_teff_data, low_teff_parameter_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_logg_array = feed_model(high_logg_data, high_logg_parameter_errors)\n",
    "medium_logg_array = feed_model(medium_logg_data, medium_logg_parameter_errors)\n",
    "low_logg_array = feed_model(low_logg_data, low_logg_parameter_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SNR Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Plot for Selecting categories\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "figure_snr = corner.corner(np.array(high_snr_array)[:, [1,2,3,4,5]], color=\"green\", labels=['VAE 1','VAE 2', 'VAE 3','VAE 4','VAE 5','VAE 6'])\n",
    "corner.corner(np.array(medium_snr_array)[:, [1,2,3,4,5]], color=\"yellow\", fig=figure_snr)\n",
    "corner.corner(np.array(low_snr_array)[:, [1,2,3,4,5]], color=\"red\", fig=figure_snr)\n",
    "#plt.title(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "\n",
    "figure_snr.suptitle(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='green', label='High', markerfacecolor='green', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='yellow', label='Medium', markerfacecolor='yellow', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='red', label='Low', markerfacecolor='red', markersize=10)]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_snr = corner.corner(np.array(high_snr_array), color=\"green\")\n",
    "corner.corner(np.array(medium_snr_array), color=\"yellow\", fig=figure_snr)\n",
    "corner.corner(np.array(low_snr_array), color=\"red\", fig=figure_snr)\n",
    "plt.title(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effective Temp Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_teff = corner.corner(np.array(high_teff_array), color=\"green\", labels=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31])\n",
    "corner.corner(np.array(medium_teff_array), color=\"yellow\", fig=figure_teff)\n",
    "corner.corner(np.array(low_teff_array), color=\"red\", fig=figure_teff)\n",
    "plt.title(\"High/Medium/Low Effective Temperature Latent Space Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Surface Gravity Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_logg = corner.corner(np.array(high_logg_array), color=\"green\", labels=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31])\n",
    "corner.corner(np.array(medium_logg_array), color=\"yellow\", fig=figure_logg)\n",
    "corner.corner(np.array(low_logg_array), color=\"red\", fig=figure_logg)\n",
    "plt.title(\"High/Medium/Low Logg Latent Space Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sample testing stars with similar parameters - vary just one parameter up/down e.g. logg or teff\n",
    "Plot in latent space and try and generate tracks, see if any trends*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Track Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "input_rows = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified data fetch function for plotting tracks\n",
    "\n",
    "def fetch_data_single(df, star_spectra, teff_min, teff_max, logg_min, logg_max, snr_min, snr_max):\n",
    "    # Surpress write on copy warning\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Isolate critical columns\n",
    "    star_df = df[['APSTAR_ID', 'TEFF_SPEC', 'LOGG_SPEC', 'SNR', 'ASPCAPFLAGS', 'STARFLAGS', 'FE_H']]\n",
    "\n",
    "    #We only include stars with no bad star flags set, SNR > 200, 4000 < teff < 5500, and logg < 3.5.\n",
    "    star_df_best = star_df.loc[(star_df['TEFF_SPEC'] < teff_max) & (star_df['TEFF_SPEC'] > teff_min) & \n",
    "    (star_df['LOGG_SPEC'] < logg_max) & (star_df['LOGG_SPEC'] > logg_min) & \n",
    "    (star_df['SNR'] < snr_max) & (star_df['SNR'] > snr_min)]\n",
    "\n",
    "    # Decode byte flags into strings\n",
    "    star_df_best['ASPCAPFLAGS'] = star_df_best['ASPCAPFLAGS'].str.decode(\"utf-8\")\n",
    "\n",
    "    # Strip out stars with STAR_BAD flag\n",
    "    star_df_best = star_df_best.loc[~(star_df_best['ASPCAPFLAGS'].str.contains(\"STAR_BAD\"))]\n",
    "\n",
    "    # Modified to add 3 parameters in 'u'\n",
    "    # Extract columns with three target parameters\n",
    "    star_df_best_parameters = star_df_best[[\"TEFF_SPEC\", \"LOGG_SPEC\", \"FE_H\"]]\n",
    "    star_df_best_parameters.reset_index()\n",
    "\n",
    "    # Make a new array with the associated errors for TEFF, LOGG, FE_H\n",
    "    # Have to transform 1d numpy array to 2d array for hstack. Transpose to get row -> Column\n",
    "    parameter_errors = np.hstack((star['teff_err'][None, :].T,star['logg_err'][None, :].T, star['fe_h_err'][None, :].T))\n",
    "    star_df_best_parameters_np = star_df_best_parameters.to_numpy()\n",
    "    star_df_best_parameters_np_std_dev = star_df_best_parameters_np.std(axis=0)\n",
    "    star_df_best_parameters_np -= star_df_best_parameters_np.mean(axis=0)\n",
    "    star_df_best_parameters_np /= star_df_best_parameters_np.std(axis=0)\n",
    "\n",
    "    #print(star_df_best.head(1))\n",
    "\n",
    "    # Update the star_spectra dataframe with only 'good' indices\n",
    "    star_spectra_filtered = star_spectra[star_df_best.index]\n",
    "    star_spectra_filtered = np.hstack((star_spectra_filtered, star_df_best_parameters_np))\n",
    "\n",
    "    ### Modified March 21 2022\n",
    "    parameter_errors = parameter_errors[star_df_best.index]\n",
    "    # Divide parameter errors by std dev of data\n",
    "    parameter_errors /= star_df_best_parameters_np_std_dev\n",
    "\n",
    "    # Update masks, errors as well\n",
    "    # star_err_data = star_err_data[star_df_best.index]\n",
    "    # star_mask_data = star_mask_data[star_df_best.index]\n",
    "\n",
    "    print(\"After applying data filters \" + str(len(star_spectra_filtered)) + \" spectra remaining\")\n",
    "    \n",
    "    # Bring star_df_best to have same indices as star_spectra_filtered (reset to start from 0)\n",
    "    star_df_best_reset = star_df_best.reset_index(drop=True)\n",
    "    #print(star_df_best_reset)\n",
    "    #print(star_spectra_filtered)\n",
    "\n",
    "    # # Reduce the dataset down to a manageable size, based on input_rows hyperparameter\n",
    "    np.random.seed(random_seed)\n",
    "    random_reduced_idx = list(np.random.choice(len(star_spectra_filtered), input_rows, replace=False))\n",
    "    \n",
    "    print(\"Reduced spectra count to \" + str(input_rows))\n",
    "\n",
    "    # # Grab only spectra with indices randomly selected from above\n",
    "    #reduced_star_spectra = np.take(star_spectra, random_reduced_idx, 0)\n",
    "\n",
    "    # Modified to just take first entry\n",
    "    #reduced_star_spectra = np.array([star_spectra_filtered[0]])\n",
    "    # Original:\n",
    "    reduced_star_spectra = np.array([star_spectra_filtered[random_reduced_idx]])\n",
    "\n",
    "    ### Modified March 21 2022\n",
    "    reduced_parameter_errors = parameter_errors[random_reduced_idx]\n",
    "\n",
    "    #print(\"Reduced spectra:\",reduced_star_spectra)\n",
    "\n",
    "    # For average star calculation\n",
    "    reduced_star_df_best = star_df_best_reset.iloc[random_reduced_idx]\n",
    "    #print(\"Reduced parameters:\",reduced_star_df_best)\n",
    "    # Calculate means of paramaters\n",
    "    print(\"Mean TEFF:\",reduced_star_df_best[\"TEFF_SPEC\"].mean())\n",
    "    print(\"Mean LOGG:\",reduced_star_df_best[\"LOGG_SPEC\"].mean())\n",
    "    print(\"Mean SNR:\",reduced_star_df_best[\"SNR\"].mean())\n",
    "\n",
    "    #print(pd.DataFrame(reduced_star_spectra))\n",
    "\n",
    "    # Normalize\n",
    "    for starRow in reduced_star_spectra:\n",
    "        starRow -= starRow.min()\n",
    "        #print(starRow.max())\n",
    "        if starRow.max() == 0:\n",
    "            #print(\"Found zero max\")\n",
    "            pass\n",
    "        else:\n",
    "            starRow /= starRow.max()\n",
    "\n",
    "    #print(pd.DataFrame(reduced_star_spectra))\n",
    "\n",
    "    # Final normalized, reduced inputs\n",
    "    train_dataset = spectraDataset(reduced_star_spectra)\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(reduced_star_spectra)\n",
    "    indices = list(range(dataset_size))\n",
    "    # split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "    # print(\"Splitting dataset at\", split)\n",
    "\n",
    "    # If shuffling is enabled, use random seed to shuffle data indices\n",
    "    if shuffle_toggle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    # Generate data loaders\n",
    "    kwargs = {'num_workers': 0}\n",
    "\n",
    "    # Try without random sampling (simple split on index)\n",
    "    # train_loader = DataLoader(train_dataset[split:], batch_size=batch_size, **kwargs)\n",
    "    test_loader = DataLoader(train_dataset, batch_size = batch_size, **kwargs)\n",
    "\n",
    "    # print('Batches in train:', len(train_loader))\n",
    "    print('Batches in test:', len(test_loader))\n",
    "\n",
    "    return test_loader, reduced_parameter_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract SNR/Teff/LogG Track Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_sample_1, snr_sample_1_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3, logg_max = 3.1, snr_min = 50, snr_max = 100)\n",
    "\n",
    "snr_sample_2, snr_sample_2_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3, logg_max = 3.1, snr_min = 100, snr_max = 150)\n",
    "\n",
    "snr_sample_3, snr_sample_3_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3, logg_max = 3.1, snr_min = 150, snr_max = 200)\n",
    "\n",
    "snr_sample_4, snr_sample_4_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3, logg_max = 3.1, snr_min = 250, snr_max = 300)\n",
    "\n",
    "snr_sample_5, snr_sample_5_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3, logg_max = 3.1, snr_min = 300, snr_max = math.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teff_sample_1 = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4200, \n",
    "# teff_max = 4400, logg_min = 3.45, logg_max = 3.5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "# teff_sample_2 = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4400, \n",
    "# teff_max = 4600, logg_min = 3.45, logg_max = 3.5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "# teff_sample_3 = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4600, \n",
    "# teff_max = 4800, logg_min = 3.45, logg_max = 3.5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "# teff_sample_4 = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "# teff_max = 5000, logg_min = 3.45, logg_max = 3.5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "# teff_sample_5 = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 5000, \n",
    "# teff_max = 5200, logg_min = 3.45, logg_max = 3.5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "# teff_sample_6 = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 5200, \n",
    "# teff_max = 5400, logg_min = 3.45, logg_max = 3.5, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "teff_sample_1, teff_sample_1_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4700, \n",
    "teff_max = 4800, logg_min = 3, logg_max = 3.1, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "teff_sample_2, teff_sample_2_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3, logg_max = 3.1, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "teff_sample_3, teff_sample_3_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4900, \n",
    "teff_max = 5000, logg_min = 3, logg_max = 3.1, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "teff_sample_4, teff_sample_4_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 5000, \n",
    "teff_max = 5100, logg_min = 3, logg_max = 3.1, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "teff_sample_5, teff_sample_5_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 5100, \n",
    "teff_max = 5200, logg_min = 3, logg_max = 3.1, snr_min = 200, snr_max = math.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logg_sample_1, logg_sample_1_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 5000, logg_min = 3, logg_max = 3.1, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "logg_sample_2, logg_sample_2_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 5000, logg_min = 3.1, logg_max = 3.2, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "logg_sample_3, logg_sample_3_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 5000, logg_min = 3.2, logg_max = 3.3, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "logg_sample_4, logg_sample_4_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 5000, logg_min = 3.3, logg_max = 3.4, snr_min = 200, snr_max = math.inf)\n",
    "\n",
    "logg_sample_5, logg_sample_5_error = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 5000, logg_min = 3.4, logg_max = 3.5, snr_min = 200, snr_max = math.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(teff_sample_1):\n",
    "    print(idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed model helper function\n",
    "def feed_model_single(data_loader, parameter_errors):\n",
    "  latent_vector_array = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(data_loader)):\n",
    "      #print(x)\n",
    "      x = x.view(batch_size, x_dim+3)\n",
    "      x = x.to(DEVICE)    \n",
    "      x_hat, mean, log_var, z = model(x, batch_idx, batch_size, parameter_errors)\n",
    "\n",
    "      # Debug latent space print\n",
    "      #print(z)\n",
    "\n",
    "      latent_vector_array += [z.cpu().detach().numpy()]\n",
    "\n",
    "  return latent_vector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_sample_1_array = feed_model_single(snr_sample_1, snr_sample_1_error)\n",
    "snr_sample_2_array = feed_model_single(snr_sample_2, snr_sample_2_error)\n",
    "snr_sample_3_array = feed_model_single(snr_sample_3, snr_sample_3_error)\n",
    "snr_sample_4_array = feed_model_single(snr_sample_4, snr_sample_4_error)\n",
    "snr_sample_5_array = feed_model_single(snr_sample_5, snr_sample_5_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teff_sample_1_array = feed_model_single(teff_sample_1, teff_sample_1_error)\n",
    "teff_sample_2_array = feed_model_single(teff_sample_2, teff_sample_2_error)\n",
    "teff_sample_3_array = feed_model_single(teff_sample_3, teff_sample_3_error)\n",
    "teff_sample_4_array = feed_model_single(teff_sample_4, teff_sample_4_error)\n",
    "teff_sample_5_array = feed_model_single(teff_sample_5, teff_sample_5_error)\n",
    "# teff_sample_6_array = feed_model_single(teff_sample_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logg_sample_1_array = feed_model_single(logg_sample_1, logg_sample_1_error)\n",
    "logg_sample_2_array = feed_model_single(logg_sample_2, logg_sample_2_error)\n",
    "logg_sample_3_array = feed_model_single(logg_sample_3, logg_sample_3_error)\n",
    "logg_sample_4_array = feed_model_single(logg_sample_4, logg_sample_4_error)\n",
    "logg_sample_5_array = feed_model_single(logg_sample_5, logg_sample_5_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average latent space values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 rows, each row with 28 latent space vals\n",
    "print(\"Number of samples:\",len(teff_sample_1_array[0]))\n",
    "print(\"Number of vals in single sample:\",len(teff_sample_1_array[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging helper function\n",
    "def latentValAverager(input):\n",
    "    # multiple_lists = [[2,5,1,9], [4,9,5,10]]\n",
    "    arrays = [np.array(x) for x in input]\n",
    "    return [np.mean(k) for k in zip(*arrays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent space values averaged across 10 samples from each class - raw list of 28 outputs\n",
    "snr_sample_1_array_mean = latentValAverager(snr_sample_1_array[0]) \n",
    "snr_sample_2_array_mean = latentValAverager(snr_sample_2_array[0]) \n",
    "snr_sample_3_array_mean = latentValAverager(snr_sample_3_array[0]) \n",
    "snr_sample_4_array_mean = latentValAverager(snr_sample_4_array[0]) \n",
    "snr_sample_5_array_mean = latentValAverager(snr_sample_5_array[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent space values averaged across 10 samples from each class - raw list of 28 outputs\n",
    "teff_sample_1_array_mean = latentValAverager(teff_sample_1_array[0]) \n",
    "teff_sample_2_array_mean = latentValAverager(teff_sample_2_array[0]) \n",
    "teff_sample_3_array_mean = latentValAverager(teff_sample_3_array[0]) \n",
    "teff_sample_4_array_mean = latentValAverager(teff_sample_4_array[0]) \n",
    "teff_sample_5_array_mean = latentValAverager(teff_sample_5_array[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent space values averaged across 10 samples from each class - raw list of 28 outputs\n",
    "logg_sample_1_array_mean = latentValAverager(logg_sample_1_array[0]) \n",
    "logg_sample_2_array_mean = latentValAverager(logg_sample_2_array[0]) \n",
    "logg_sample_3_array_mean = latentValAverager(logg_sample_3_array[0]) \n",
    "logg_sample_4_array_mean = latentValAverager(logg_sample_4_array[0]) \n",
    "logg_sample_5_array_mean = latentValAverager(logg_sample_5_array[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SNR Tracking Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 31\n",
    "# Extract the axes\n",
    "axes = np.array(figure_snr.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "\n",
    "        ax.plot(snr_sample_1_array_mean[xi], snr_sample_1_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(snr_sample_2_array_mean[xi], snr_sample_2_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(snr_sample_3_array_mean[xi], snr_sample_3_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(snr_sample_4_array_mean[xi], snr_sample_4_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(snr_sample_5_array_mean[xi], snr_sample_5_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "\n",
    "        # Add text label\n",
    "        ax.annotate(\"1\",xy=(snr_sample_1_array_mean[xi], snr_sample_1_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"2\",xy=(snr_sample_2_array_mean[xi], snr_sample_2_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"3\",xy=(snr_sample_3_array_mean[xi], snr_sample_3_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"4\",xy=(snr_sample_4_array_mean[xi], snr_sample_4_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"5\",xy=(snr_sample_5_array_mean[xi], snr_sample_5_array_mean[yi]), fontsize=8)\n",
    "\n",
    "figure_snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Plot for Selecting categories\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "figure_snr_crop = corner.corner(np.array(high_snr_array)[:, [1,2,3,4,5]], color=\"green\", labels=['VAE 1','VAE 2', 'VAE 3','VAE 4','VAE 5','VAE 6'])\n",
    "corner.corner(np.array(medium_snr_array)[:, [1,2,3,4,5]], color=\"yellow\", fig=figure_snr_crop)\n",
    "corner.corner(np.array(low_snr_array)[:, [1,2,3,4,5]], color=\"red\", fig=figure_snr_crop)\n",
    "#plt.title(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "\n",
    "figure_snr_crop.suptitle(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='green', label='High', markerfacecolor='green', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='yellow', label='Medium', markerfacecolor='yellow', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='red', label='Low', markerfacecolor='red', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='blue', label='Track', markerfacecolor='blue', markersize=10)\n",
    "                   ]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.25, 2))\n",
    "\n",
    "ndim = 5\n",
    "# Extract the axes\n",
    "axes = np.array(figure_snr_crop.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        #print(\"ypos:\",yi+1,\"xpos:\",xi+1)\n",
    "\n",
    "        # ax.text(0.9, 0.95,(xi+1,yi+1),\n",
    "        # horizontalalignment='center',\n",
    "        # verticalalignment='center',\n",
    "        # transform = ax.transAxes)\n",
    "        # ax.annotate((xi+1,yi+1), xy=(0,0))\n",
    "\n",
    "        ax.plot(snr_sample_1_array_mean[xi+1], snr_sample_1_array_mean[yi+1], marker=\"o\", markersize=4, color=\"blue\")\n",
    "        ax.plot(snr_sample_2_array_mean[xi+1], snr_sample_2_array_mean[yi+1], marker=\"o\", markersize=4, color=\"blue\")\n",
    "        ax.plot(snr_sample_3_array_mean[xi+1], snr_sample_3_array_mean[yi+1], marker=\"o\", markersize=4, color=\"blue\")\n",
    "        ax.plot(snr_sample_4_array_mean[xi+1], snr_sample_4_array_mean[yi+1], marker=\"o\", markersize=4, color=\"blue\")\n",
    "        ax.plot(snr_sample_5_array_mean[xi+1], snr_sample_5_array_mean[yi+1], marker=\"o\", markersize=4, color=\"blue\")\n",
    "\n",
    "        # Add text label\n",
    "        # ax.annotate(\"1\",xy=(snr_sample_1_array_mean[xi+1], snr_sample_1_array_mean[yi+1]), fontsize=8)\n",
    "        # ax.annotate(\"2\",xy=(snr_sample_2_array_mean[xi+1], snr_sample_2_array_mean[yi+1]), fontsize=8)\n",
    "        # ax.annotate(\"3\",xy=(snr_sample_3_array_mean[xi+1], snr_sample_3_array_mean[yi+1]), fontsize=8)\n",
    "        # ax.annotate(\"4\",xy=(snr_sample_4_array_mean[xi+1], snr_sample_4_array_mean[yi+1]), fontsize=8)\n",
    "        # ax.annotate(\"5\",xy=(snr_sample_5_array_mean[xi+1], snr_sample_5_array_mean[yi+1]), fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effective Temp Tracking Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 31\n",
    "\n",
    "# ndim, nsamples = ndim, 50\n",
    "# np.random.seed(42)\n",
    "# samples = np.random.randn(ndim * nsamples).reshape([nsamples, ndim])\n",
    "# figure = corner.corner(samples)\n",
    "\n",
    "# Extract the axes\n",
    "axes = np.array(figure_teff.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Loop over the diagonal - histograms\n",
    "# for i in range(ndim):\n",
    "\n",
    "#     ax = axes[i, i]\n",
    "#     ax.axvline(1, color=\"g\")\n",
    "#     ax.axvline(1, color=\"r\")\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        # ax.annotate(str(yi)+\",\"+str(xi), (teff_sample_1_array[0][0][xi], teff_sample_1_array[0][0][yi]))\n",
    "\n",
    "        ax.plot(teff_sample_1_array_mean[xi], teff_sample_1_array_mean[yi], marker=\"o\", markersize=4, color=\"purple\")\n",
    "        ax.plot(teff_sample_2_array_mean[xi], teff_sample_2_array_mean[yi], marker=\"o\", markersize=4, color=\"purple\")\n",
    "        ax.plot(teff_sample_3_array_mean[xi], teff_sample_3_array_mean[yi], marker=\"o\", markersize=4, color=\"purple\")\n",
    "        ax.plot(teff_sample_4_array_mean[xi], teff_sample_4_array_mean[yi], marker=\"o\", markersize=4, color=\"purple\")\n",
    "        ax.plot(teff_sample_5_array_mean[xi], teff_sample_5_array_mean[yi], marker=\"o\", markersize=4, color=\"purple\")\n",
    "\n",
    "        # Add text label\n",
    "        ax.annotate(\"1\",xy=(teff_sample_1_array_mean[xi], teff_sample_1_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"2\",xy=(teff_sample_2_array_mean[xi], teff_sample_2_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"3\",xy=(teff_sample_3_array_mean[xi], teff_sample_3_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"4\",xy=(teff_sample_4_array_mean[xi], teff_sample_4_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"5\",xy=(teff_sample_5_array_mean[xi], teff_sample_5_array_mean[yi]), fontsize=8)\n",
    "\n",
    "        # Old plotting code for single sample\n",
    "        # ax.plot(teff_sample_6_array[0][0][xi], teff_sample_6_array[0][0][yi], marker=\".\", markersize=10, color=\"lavender\")\n",
    "\n",
    "        # ax.plot(1, 1, \"sg\")\n",
    "        # ax.plot(2, 2, \"sr\")\n",
    "\n",
    "figure_teff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Plot for Selecting categories\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "figure_teff_crop = corner.corner(np.array(high_teff_array)[:, [8,9,10,11,12]], color=\"green\", labels=['VAE 9','VAE 10','VAE 11', 'VAE 12','VAE 13'])\n",
    "corner.corner(np.array(medium_teff_array)[:, [8,9,10,11,12]], color=\"yellow\", fig=figure_teff_crop)\n",
    "corner.corner(np.array(low_teff_array)[:, [8,9,10,11,12]], color=\"red\", fig=figure_teff_crop)\n",
    "#plt.title(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "\n",
    "figure_teff_crop.suptitle(\"High/Medium/Low Effective Temperature Latent Space Comparison\")\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='green', label='High', markerfacecolor='green', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='yellow', label='Medium', markerfacecolor='yellow', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='red', label='Low', markerfacecolor='red', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='blue', label='Track', markerfacecolor='dodgerblue', markersize=10)\n",
    "                   ]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.25, 2))\n",
    "\n",
    "ndim = 5\n",
    "# Extract the axes\n",
    "axes = np.array(figure_teff_crop.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Offset = first index in crop\n",
    "offset = 8\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        #print(\"ypos:\",yi+1,\"xpos:\",xi+1)\n",
    "\n",
    "        # ax.text(0.9, 0.95,(xi,yi),\n",
    "        # horizontalalignment='center',\n",
    "        # verticalalignment='center',\n",
    "        # transform = ax.transAxes)\n",
    "        # ax.annotate((xi+1,yi+1), xy=(0,0))\n",
    "\n",
    "        ax.plot(teff_sample_1_array_mean[xi+offset], teff_sample_1_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=3, color=\"blue\")\n",
    "        ax.plot(teff_sample_2_array_mean[xi+offset], teff_sample_2_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=3, color=\"blue\")\n",
    "        ax.plot(teff_sample_3_array_mean[xi+offset], teff_sample_3_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=3, color=\"blue\")\n",
    "        ax.plot(teff_sample_4_array_mean[xi+offset], teff_sample_4_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=3, color=\"blue\")\n",
    "        ax.plot(teff_sample_5_array_mean[xi+offset], teff_sample_5_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=3, color=\"blue\")\n",
    "\n",
    "        # Add text label\n",
    "        ax.annotate(\"1\",xy=(teff_sample_1_array_mean[xi+offset], teff_sample_1_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"2\",xy=(teff_sample_2_array_mean[xi+offset], teff_sample_2_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"3\",xy=(teff_sample_3_array_mean[xi+offset], teff_sample_3_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"4\",xy=(teff_sample_4_array_mean[xi+offset], teff_sample_4_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"5\",xy=(teff_sample_5_array_mean[xi+offset], teff_sample_5_array_mean[yi+offset]), fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Surface Gravity Tracking Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 31\n",
    "\n",
    "# ndim, nsamples = ndim, 50\n",
    "# np.random.seed(42)\n",
    "# samples = np.random.randn(ndim * nsamples).reshape([nsamples, ndim])\n",
    "# figure = corner.corner(samples)\n",
    "\n",
    "# Extract the axes\n",
    "axes = np.array(figure_logg.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Loop over the diagonal - histograms\n",
    "# for i in range(ndim):\n",
    "\n",
    "#     ax = axes[i, i]\n",
    "#     ax.axvline(1, color=\"g\")\n",
    "#     ax.axvline(1, color=\"r\")\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        # ax.annotate(str(yi)+\",\"+str(xi), (teff_sample_1_array[0][0][xi], teff_sample_1_array[0][0][yi]))\n",
    "\n",
    "        ax.plot(logg_sample_1_array_mean[xi], logg_sample_1_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(logg_sample_2_array_mean[xi], logg_sample_2_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(logg_sample_3_array_mean[xi], logg_sample_3_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(logg_sample_4_array_mean[xi], logg_sample_4_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(logg_sample_5_array_mean[xi], logg_sample_5_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "\n",
    "        # Add text label\n",
    "        ax.annotate(\"1\",xy=(logg_sample_1_array_mean[xi], logg_sample_1_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"2\",xy=(logg_sample_2_array_mean[xi], logg_sample_2_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"3\",xy=(logg_sample_3_array_mean[xi], logg_sample_3_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"4\",xy=(logg_sample_4_array_mean[xi], logg_sample_4_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"5\",xy=(logg_sample_5_array_mean[xi], logg_sample_5_array_mean[yi]), fontsize=8)\n",
    "\n",
    "        # Old plotting code for single sample\n",
    "        # ax.plot(teff_sample_6_array[0][0][xi], teff_sample_6_array[0][0][yi], marker=\".\", markersize=10, color=\"lavender\")\n",
    "\n",
    "        # ax.plot(1, 1, \"sg\")\n",
    "        # ax.plot(2, 2, \"sr\")\n",
    "\n",
    "figure_logg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Plot for Selecting categories\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "figure_logg_crop = corner.corner(np.array(high_logg_array)[:, [16,17,18,19,20]], color=\"green\", labels=['VAE 17','VAE 18','VAE 19', 'VAE 20','VAE 21'])\n",
    "corner.corner(np.array(medium_logg_array)[:, [16,17,18,19,20]], color=\"yellow\", fig=figure_logg_crop)\n",
    "corner.corner(np.array(low_logg_array)[:, [16,17,18,19,20]], color=\"red\", fig=figure_logg_crop)\n",
    "#plt.title(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "\n",
    "figure_logg_crop.suptitle(\"High/Medium/Low Surface Gravity Latent Space Comparison\")\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='green', label='High', markerfacecolor='green', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='yellow', label='Medium', markerfacecolor='yellow', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='red', label='Low', markerfacecolor='red', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='blue', label='Track', markerfacecolor='dodgerblue', markersize=10)\n",
    "                   ]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.25, 2))\n",
    "\n",
    "ndim = 5\n",
    "# Extract the axes\n",
    "axes = np.array(figure_logg_crop.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Offset = first index in crop\n",
    "offset = 16\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        #print(\"ypos:\",yi+1,\"xpos:\",xi+1)\n",
    "\n",
    "        # ax.text(0.15, 0.15,(xi,yi),\n",
    "        # horizontalalignment='center',\n",
    "        # verticalalignment='center',\n",
    "        # transform = ax.transAxes)\n",
    "        # ax.annotate((xi+1,yi+1), xy=(0,0))\n",
    "\n",
    "        ax.plot(logg_sample_1_array_mean[xi+offset], logg_sample_1_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(logg_sample_2_array_mean[xi+offset], logg_sample_2_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(logg_sample_3_array_mean[xi+offset], logg_sample_3_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(logg_sample_4_array_mean[xi+offset], logg_sample_4_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\", markersize=4, color=\"blue\")\n",
    "        ax.plot(logg_sample_5_array_mean[xi+offset], logg_sample_5_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "\n",
    "        # Add text label\n",
    "        ax.annotate(\"1\",xy=(logg_sample_1_array_mean[xi+offset], logg_sample_1_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"2\",xy=(logg_sample_2_array_mean[xi+offset], logg_sample_2_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"3\",xy=(logg_sample_3_array_mean[xi+offset], logg_sample_3_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"4\",xy=(logg_sample_4_array_mean[xi+offset], logg_sample_4_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"5\",xy=(logg_sample_5_array_mean[xi+offset], logg_sample_5_array_mean[yi+offset]), fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FE_H,  O_FE Corner Plots - Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified data fetch function for metallicity/abundance\n",
    "\n",
    "def fetch_data_abundance(df, star_spectra, teff_min, teff_max, logg_min, logg_max, snr_min, snr_max, \n",
    "metallicity_min=None, metallicity_max=None, abundance_min=None, abundance_max=None, abundance_flag=None):\n",
    "    # Surpress write on copy warning\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Isolate critical columns\n",
    "\n",
    "    # No specific chemical abundance to target\n",
    "    # Baseline: We only include stars with no bad star flags set, SNR > 200, 4000 < teff < 5500, and logg < 3.5.\n",
    "\n",
    "    if abundance_flag == None:\n",
    "        star_df = df[['APSTAR_ID', 'TEFF_SPEC', 'LOGG_SPEC', 'SNR', 'ASPCAPFLAGS', 'STARFLAGS', 'FE_H']]\n",
    "\n",
    "        star_df_best = star_df.loc[(star_df['TEFF_SPEC'] < teff_max) & (star_df['TEFF_SPEC'] > teff_min) & \n",
    "        (star_df['LOGG_SPEC'] < logg_max) & (star_df['LOGG_SPEC'] > logg_min) & \n",
    "        (star_df['SNR'] < snr_max) & (star_df['SNR'] > snr_min) &\n",
    "        (star_df['FE_H'] < metallicity_max) & (star_df['FE_H'] > metallicity_min)]\n",
    "        \n",
    "    else:\n",
    "        star_df = df[['APSTAR_ID', 'TEFF_SPEC', 'LOGG_SPEC', 'SNR', 'ASPCAPFLAGS', 'STARFLAGS', 'FE_H', abundance_flag]]\n",
    "\n",
    "        star_df_best = star_df.loc[(star_df['TEFF_SPEC'] < teff_max) & (star_df['TEFF_SPEC'] > teff_min) & \n",
    "        (star_df['LOGG_SPEC'] < logg_max) & (star_df['LOGG_SPEC'] > logg_min) & \n",
    "        (star_df['SNR'] < snr_max) & (star_df['SNR'] > snr_min) &\n",
    "        (star_df['FE_H'] < metallicity_max) & (star_df['FE_H'] > metallicity_min) &\n",
    "        (star_df[abundance_flag] < abundance_max) & (star_df[abundance_flag] > abundance_min)] \n",
    "\n",
    "    # Decode byte flags into strings\n",
    "    star_df_best['ASPCAPFLAGS'] = star_df_best['ASPCAPFLAGS'].str.decode(\"utf-8\")\n",
    "\n",
    "    # Strip out stars with STAR_BAD flag\n",
    "    star_df_best = star_df_best.loc[~(star_df_best['ASPCAPFLAGS'].str.contains(\"STAR_BAD\"))]\n",
    "\n",
    "    # Modified to add 3 parameters in 'u'\n",
    "    # Extract columns with three target parameters\n",
    "    star_df_best_parameters = star_df_best[[\"TEFF_SPEC\", \"LOGG_SPEC\", \"FE_H\"]]\n",
    "    star_df_best_parameters.reset_index()\n",
    "\n",
    "    # Make a new array with the associated errors for TEFF, LOGG, FE_H\n",
    "    # Have to transform 1d numpy array to 2d array for hstack. Transpose to get row -> Column\n",
    "    parameter_errors = np.hstack((star['teff_err'][None, :].T,star['logg_err'][None, :].T, star['fe_h_err'][None, :].T))\n",
    "    star_df_best_parameters_np = star_df_best_parameters.to_numpy()\n",
    "    star_df_best_parameters_np_std_dev = star_df_best_parameters_np.std(axis=0)\n",
    "    star_df_best_parameters_np -= star_df_best_parameters_np.mean(axis=0)\n",
    "    star_df_best_parameters_np /= star_df_best_parameters_np.std(axis=0)\n",
    "\n",
    "    #print(star_df_best.head(1))\n",
    "\n",
    "    # Update the star_spectra dataframe with only 'good' indices\n",
    "    star_spectra_filtered = star_spectra[star_df_best.index]\n",
    "    star_spectra_filtered = np.hstack((star_spectra_filtered, star_df_best_parameters_np))\n",
    "\n",
    "    ### Modified March 21 2022\n",
    "    parameter_errors = parameter_errors[star_df_best.index]\n",
    "    # Divide parameter errors by std dev of data\n",
    "    parameter_errors /= star_df_best_parameters_np_std_dev\n",
    "    # Update masks, errors as well\n",
    "    # star_err_data = star_err_data[star_df_best.index]\n",
    "    # star_mask_data = star_mask_data[star_df_best.index]\n",
    "\n",
    "    print(\"After applying data filters \" + str(len(star_spectra_filtered)) + \" spectra remaining\")\n",
    "\n",
    "     # Bring star_df_best to have same indices as star_spectra_filtered (reset to start from 0)\n",
    "    star_df_best_reset = star_df_best.reset_index(drop=True)\n",
    "    #print(star_df_best_reset)\n",
    "    #print(star_spectra_filtered)\n",
    "\n",
    "    # # Reduce the dataset down to a manageable size, based on input_rows hyperparameter\n",
    "    np.random.seed(random_seed)\n",
    "    random_reduced_idx = list(np.random.choice(len(star_spectra_filtered), input_rows, replace=False))\n",
    "    \n",
    "    print(\"Reduced spectra count to \" + str(input_rows))\n",
    "\n",
    "    # # Grab only spectra with indices randomly selected from above\n",
    "    #reduced_star_spectra = np.take(star_spectra, random_reduced_idx, 0)\n",
    "\n",
    "    # Modified to just take first entry\n",
    "    #reduced_star_spectra = np.array([star_spectra_filtered[0]])\n",
    "    # Original:\n",
    "    reduced_star_spectra = star_spectra_filtered[random_reduced_idx]\n",
    "\n",
    "    ### Modified March 21 2022\n",
    "    reduced_parameter_errors = parameter_errors[random_reduced_idx]\n",
    "\n",
    "    #print(\"Reduced spectra:\",reduced_star_spectra)\n",
    "\n",
    "    # For average star calculation\n",
    "    reduced_star_df_best = star_df_best_reset.iloc[random_reduced_idx]\n",
    "    #print(\"Reduced parameters:\",reduced_star_df_best)\n",
    "    # Calculate means of paramaters\n",
    "    print(\"Mean TEFF:\",reduced_star_df_best[\"TEFF_SPEC\"].mean())\n",
    "    print(\"Mean LOGG:\",reduced_star_df_best[\"LOGG_SPEC\"].mean())\n",
    "    print(\"Mean SNR:\",reduced_star_df_best[\"SNR\"].mean())\n",
    "    print(\"Mean Fe/H:\",reduced_star_df_best[\"FE_H\"].mean())\n",
    "\n",
    "    if abundance_flag != None:\n",
    "        print(\"Mean Abundance:\",reduced_star_df_best[abundance_flag].mean())\n",
    "\n",
    "    # Normalize\n",
    "    for starRow in reduced_star_spectra:\n",
    "        starRow -= starRow.min()\n",
    "        #print(starRow.max())\n",
    "        if starRow.max() == 0:\n",
    "            #print(\"Found zero max\")\n",
    "            pass\n",
    "        else:\n",
    "            starRow /= starRow.max()\n",
    "\n",
    "    #print(pd.DataFrame(reduced_star_spectra))\n",
    "\n",
    "    # Final normalized, reduced inputs\n",
    "    train_dataset = spectraDataset(reduced_star_spectra)\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(reduced_star_spectra)\n",
    "    indices = list(range(dataset_size))\n",
    "    # split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "    # print(\"Splitting dataset at\", split)\n",
    "\n",
    "    # If shuffling is enabled, use random seed to shuffle data indices\n",
    "    if shuffle_toggle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    # Generate data loaders\n",
    "    kwargs = {'num_workers': 0}\n",
    "\n",
    "    # Try without random sampling (simple split on index)\n",
    "    # train_loader = DataLoader(train_dataset[split:], batch_size=batch_size, **kwargs)\n",
    "    test_loader = DataLoader(train_dataset, batch_size=batch_size, **kwargs)\n",
    "\n",
    "    # print('Batches in train:', len(train_loader))\n",
    "    print('Batches in test:', len(test_loader))\n",
    "\n",
    "    return test_loader, reduced_parameter_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metallicity\n",
    "\n",
    "input_rows = 10000\n",
    "batch_size = 100\n",
    "\n",
    "low_fe_h_data, low_fe_h_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 200, snr_max = math.inf, metallicity_min = -0.5, metallicity_max = -0.25)\n",
    "\n",
    "medium_fe_h_data, medium_fe_h_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 200, snr_max = math.inf, metallicity_min = -0.25, metallicity_max = 0)\n",
    "\n",
    "high_fe_h_data, high_fe_h_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 200, snr_max = math.inf, metallicity_min = 0, metallicity_max = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oxygen Abundance\n",
    "\n",
    "input_rows = 10000\n",
    "batch_size = 100\n",
    "\n",
    "low_o_fe_data, low_o_fe_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 200, snr_max = math.inf, metallicity_min = -0.25,\n",
    "metallicity_max = 0.25, abundance_min = -math.inf, abundance_max = 0, abundance_flag='O_FE')\n",
    "\n",
    "medium_o_fe_data, medium_o_fe_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 200, snr_max = math.inf, metallicity_min = -0.25, \n",
    "metallicity_max = 0.25, abundance_min = 0, abundance_max = 0.05, abundance_flag='O_FE')\n",
    "\n",
    "high_o_fe_data, high_o_fe_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 0, logg_max = 3.5, snr_min = 200, snr_max = math.inf, metallicity_min = -0.25,\n",
    "metallicity_max = 0.25, abundance_min = 0.05, abundance_max = 0.2, abundance_flag='O_FE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_fe_h_array = feed_model(high_fe_h_data, high_fe_h_errors)\n",
    "medium_fe_h_array = feed_model(medium_fe_h_data, medium_fe_h_errors)\n",
    "low_fe_h_array = feed_model(low_fe_h_data, low_fe_h_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_o_fe_array = feed_model(high_o_fe_data, high_o_fe_errors)\n",
    "medium_o_fe_array = feed_model(medium_o_fe_data, medium_o_fe_errors)\n",
    "low_o_fe_array = feed_model(low_o_fe_data, low_o_fe_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fe/H, O/Fe Track Sample Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tracks, average out plotted points across 50 samples\n",
    "input_rows = 50\n",
    "batch_size = 50\n",
    "\n",
    "fe_h_sample_1, fe_h_sample_1_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -0.5, metallicity_max = -0.4)\n",
    "\n",
    "fe_h_sample_2, fe_h_sample_2_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -0.4, metallicity_max = -0.3)\n",
    "\n",
    "fe_h_sample_3, fe_h_sample_3_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -0.3, metallicity_max = -0.2)\n",
    "\n",
    "fe_h_sample_4, fe_h_sample_4_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -0.2, metallicity_max = -0.1)\n",
    "\n",
    "fe_h_sample_5, fe_h_sample_5_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -0.1, metallicity_max = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tracks, average out plotted points across 50 samples\n",
    "input_rows = 50\n",
    "batch_size = 50\n",
    "\n",
    "# Metallicity originally between -0.3, 0.1\n",
    "\n",
    "o_fe_sample_1, o_fe_sample_1_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "abundance_min = -math.inf, abundance_max = -0.5, abundance_flag='O_FE')\n",
    "\n",
    "o_fe_sample_2, o_fe_sample_2_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "abundance_min = -0.5, abundance_max = 0, abundance_flag='O_FE')\n",
    "\n",
    "o_fe_sample_3, o_fe_sample_3_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "abundance_min = 0, abundance_max = 0.25, abundance_flag='O_FE')\n",
    "\n",
    "# Scales are not evenly increasing at this point - didn't have enough samples\n",
    "\n",
    "o_fe_sample_4, o_fe_sample_4_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "abundance_min = 0.25, abundance_max = 0.35, abundance_flag='O_FE')\n",
    "\n",
    "o_fe_sample_5, o_fe_sample_5_errors = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4000, \n",
    "teff_max = 5500, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "abundance_min = 0.35, abundance_max = math.inf, abundance_flag='O_FE')\n",
    "\n",
    "\n",
    "\n",
    "# o_fe_sample_1 = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "# teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "# abundance_min = 0, abundance_max = 0.025, abundance_flag='O_FE')\n",
    "\n",
    "# o_fe_sample_2 = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "# teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "# abundance_min = 0.025, abundance_max = 0.05, abundance_flag='O_FE')\n",
    "\n",
    "# o_fe_sample_3 = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "# teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "# abundance_min = 0.05, abundance_max = 0.075, abundance_flag='O_FE')\n",
    "\n",
    "# # Scales are not evenly increasing at this point - didn't have enough samples\n",
    "\n",
    "# o_fe_sample_4 = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "# teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "# abundance_min = 0.075, abundance_max = 0.12, abundance_flag='O_FE')\n",
    "\n",
    "# o_fe_sample_5 = fetch_data_abundance(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "# teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf, metallicity_min = -math.inf, metallicity_max = math.inf,\n",
    "# abundance_min = 0.12, abundance_max = math.inf, abundance_flag='O_FE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_h_sample_1_array = feed_model_single(fe_h_sample_1, fe_h_sample_1_errors)\n",
    "fe_h_sample_2_array = feed_model_single(fe_h_sample_2, fe_h_sample_2_errors)\n",
    "fe_h_sample_3_array = feed_model_single(fe_h_sample_3, fe_h_sample_3_errors)\n",
    "fe_h_sample_4_array = feed_model_single(fe_h_sample_4, fe_h_sample_4_errors)\n",
    "fe_h_sample_5_array = feed_model_single(fe_h_sample_5, fe_h_sample_5_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_fe_sample_1_array = feed_model_single(o_fe_sample_1, o_fe_sample_1_errors)\n",
    "o_fe_sample_2_array = feed_model_single(o_fe_sample_2, o_fe_sample_2_errors)\n",
    "o_fe_sample_3_array = feed_model_single(o_fe_sample_3, o_fe_sample_3_errors)\n",
    "o_fe_sample_4_array = feed_model_single(o_fe_sample_4, o_fe_sample_4_errors)\n",
    "o_fe_sample_5_array = feed_model_single(o_fe_sample_5, o_fe_sample_5_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent space values averaged across 10 samples from each class - raw list of 28 outputs\n",
    "fe_h_sample_1_array_mean = latentValAverager(fe_h_sample_1_array[0]) \n",
    "fe_h_sample_2_array_mean = latentValAverager(fe_h_sample_2_array[0]) \n",
    "fe_h_sample_3_array_mean = latentValAverager(fe_h_sample_3_array[0]) \n",
    "fe_h_sample_4_array_mean = latentValAverager(fe_h_sample_4_array[0]) \n",
    "fe_h_sample_5_array_mean = latentValAverager(fe_h_sample_5_array[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent space values averaged across 10 samples from each class - raw list of 28 outputs\n",
    "o_fe_sample_1_array_mean = latentValAverager(o_fe_sample_1_array[0]) \n",
    "o_fe_sample_2_array_mean = latentValAverager(o_fe_sample_2_array[0]) \n",
    "o_fe_sample_3_array_mean = latentValAverager(o_fe_sample_3_array[0]) \n",
    "o_fe_sample_4_array_mean = latentValAverager(o_fe_sample_4_array[0]) \n",
    "o_fe_sample_5_array_mean = latentValAverager(o_fe_sample_5_array[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metallicity Track Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_fe_h = corner.corner(np.array(high_fe_h_array), color=\"green\", labels=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31])\n",
    "corner.corner(np.array(medium_fe_h_array), color=\"yellow\", fig=figure_fe_h)\n",
    "corner.corner(np.array(low_fe_h_array), color=\"red\", fig=figure_fe_h)\n",
    "plt.title(\"High/Medium/Low Metallicity (Fe/H) Latent Space Comparison\")\n",
    "\n",
    "ndim = 31\n",
    "\n",
    "axes = np.array(figure_fe_h.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "  \n",
    "        ax.plot(fe_h_sample_1_array_mean[xi], fe_h_sample_1_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(fe_h_sample_2_array_mean[xi], fe_h_sample_2_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(fe_h_sample_3_array_mean[xi], fe_h_sample_3_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(fe_h_sample_4_array_mean[xi], fe_h_sample_4_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(fe_h_sample_5_array_mean[xi], fe_h_sample_5_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "\n",
    "        # Add text label\n",
    "        ax.annotate(\"1\",xy=(fe_h_sample_1_array_mean[xi], fe_h_sample_1_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"2\",xy=(fe_h_sample_2_array_mean[xi], fe_h_sample_2_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"3\",xy=(fe_h_sample_3_array_mean[xi], fe_h_sample_3_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"4\",xy=(fe_h_sample_4_array_mean[xi], fe_h_sample_4_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"5\",xy=(fe_h_sample_5_array_mean[xi], fe_h_sample_5_array_mean[yi]), fontsize=8)\n",
    "\n",
    "figure_fe_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Plot for Selecting categories\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "figure_fe_h_crop = corner.corner(np.array(high_fe_h_array)[:, [16,17,18,19,20]], color=\"green\", labels=['VAE 17','VAE 18','VAE 19', 'VAE 20','VAE 21'])\n",
    "corner.corner(np.array(medium_fe_h_array)[:, [16,17,18,19,20]], color=\"yellow\", fig=figure_fe_h_crop)\n",
    "corner.corner(np.array(low_fe_h_array)[:, [16,17,18,19,20]], color=\"red\", fig=figure_fe_h_crop)\n",
    "#plt.title(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "\n",
    "figure_fe_h_crop.suptitle(\"High/Medium/Low [Fe/H] Latent Space Comparison\")\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='green', label='High', markerfacecolor='green', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='yellow', label='Medium', markerfacecolor='yellow', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='red', label='Low', markerfacecolor='red', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='blue', label='Track', markerfacecolor='dodgerblue', markersize=10)\n",
    "                   ]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.25, 2))\n",
    "\n",
    "ndim = 5\n",
    "# Extract the axes\n",
    "axes = np.array(figure_fe_h_crop.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Offset = first index in crop\n",
    "offset = 16\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        #print(\"ypos:\",yi+1,\"xpos:\",xi+1)\n",
    "\n",
    "        # ax.text(0.15, 0.15,(xi,yi),\n",
    "        # horizontalalignment='center',\n",
    "        # verticalalignment='center',\n",
    "        # transform = ax.transAxes)\n",
    "        # ax.annotate((xi+1,yi+1), xy=(0,0))\n",
    "\n",
    "        ax.plot(fe_h_sample_1_array_mean[xi+offset], fe_h_sample_1_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(fe_h_sample_2_array_mean[xi+offset], fe_h_sample_2_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(fe_h_sample_3_array_mean[xi+offset], fe_h_sample_3_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(fe_h_sample_4_array_mean[xi+offset], fe_h_sample_4_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(fe_h_sample_5_array_mean[xi+offset], fe_h_sample_5_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "\n",
    "        # Add text label\n",
    "        ax.annotate(\"1\",xy=(fe_h_sample_1_array_mean[xi+offset], fe_h_sample_1_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"2\",xy=(fe_h_sample_2_array_mean[xi+offset], fe_h_sample_2_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"3\",xy=(fe_h_sample_3_array_mean[xi+offset], fe_h_sample_3_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"4\",xy=(fe_h_sample_4_array_mean[xi+offset], fe_h_sample_4_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"5\",xy=(fe_h_sample_5_array_mean[xi+offset], fe_h_sample_5_array_mean[yi+offset]), fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oxygen Abundance Track Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_o_fe = corner.corner(np.array(high_o_fe_array), color=\"green\", labels=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31])\n",
    "corner.corner(np.array(medium_o_fe_array), color=\"yellow\", fig=figure_o_fe)\n",
    "corner.corner(np.array(low_o_fe_array), color=\"red\", fig=figure_o_fe)\n",
    "plt.title(\"High/Medium/Low Oxygen Abundance (O/Fe) Latent Space Comparison\")\n",
    "\n",
    "ndim = 31\n",
    "\n",
    "axes = np.array(figure_o_fe.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "  \n",
    "        ax.plot(o_fe_sample_1_array_mean[xi], o_fe_sample_1_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(o_fe_sample_2_array_mean[xi], o_fe_sample_2_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(o_fe_sample_3_array_mean[xi], o_fe_sample_3_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(o_fe_sample_4_array_mean[xi], o_fe_sample_4_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "        ax.plot(o_fe_sample_5_array_mean[xi], o_fe_sample_5_array_mean[yi], marker=\"o\", markersize=2, color=\"darkviolet\")\n",
    "\n",
    "        # Add text label\n",
    "        ax.annotate(\"1\",xy=(o_fe_sample_1_array_mean[xi], o_fe_sample_1_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"2\",xy=(o_fe_sample_2_array_mean[xi], o_fe_sample_2_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"3\",xy=(o_fe_sample_3_array_mean[xi], o_fe_sample_3_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"4\",xy=(o_fe_sample_4_array_mean[xi], o_fe_sample_4_array_mean[yi]), fontsize=8)\n",
    "        ax.annotate(\"5\",xy=(o_fe_sample_5_array_mean[xi], o_fe_sample_5_array_mean[yi]), fontsize=8)\n",
    "\n",
    "figure_o_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Plot for Selecting categories\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "figure_o_fe_crop = corner.corner(np.array(high_o_fe_array)[:, [13,14,15,16,17,18]], color=\"green\", labels=['VAE 14','VAE 15','VAE 16', 'VAE 17','VAE 18', 'VAE 19'])\n",
    "corner.corner(np.array(medium_o_fe_array)[:, [13,14,15,16,17,18]], color=\"yellow\", fig=figure_o_fe_crop)\n",
    "corner.corner(np.array(low_o_fe_array)[:, [13,14,15,16,17,18]], color=\"red\", fig=figure_o_fe_crop)\n",
    "#plt.title(\"High/Medium/Low SNR Latent Space Comparison\")\n",
    "\n",
    "figure_o_fe_crop.suptitle(\"High/Medium/Low [O/Fe] Latent Space Comparison\")\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='green', label='High', markerfacecolor='green', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='yellow', label='Medium', markerfacecolor='yellow', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='red', label='Low', markerfacecolor='red', markersize=10),\n",
    "                   Line2D([0], [0], marker='o', color='blue', label='Track', markerfacecolor='dodgerblue', markersize=10)\n",
    "                   ]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.25, 2))\n",
    "\n",
    "ndim = 6\n",
    "# Extract the axes\n",
    "axes = np.array(figure_o_fe_crop.axes).reshape((ndim, ndim))\n",
    "\n",
    "# Offset = first index in crop\n",
    "offset = 13\n",
    "\n",
    "# Loop over scatter plots\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        #print(\"ypos:\",yi+1,\"xpos:\",xi+1)\n",
    "\n",
    "        # ax.text(0.15, 0.15,(xi,yi),\n",
    "        # horizontalalignment='center',\n",
    "        # verticalalignment='center',\n",
    "        # transform = ax.transAxes)\n",
    "        # ax.annotate((xi+1,yi+1), xy=(0,0))\n",
    "\n",
    "        #ax.plot(o_fe_sample_1_array_mean[xi+offset], o_fe_sample_1_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(o_fe_sample_2_array_mean[xi+offset], o_fe_sample_2_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(o_fe_sample_3_array_mean[xi+offset], o_fe_sample_3_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(o_fe_sample_4_array_mean[xi+offset], o_fe_sample_4_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "        ax.plot(o_fe_sample_5_array_mean[xi+offset], o_fe_sample_5_array_mean[yi+offset], marker=\"o\", markerfacecolor=\"dodgerblue\",markersize=4, color=\"blue\")\n",
    "\n",
    "        # Add text label\n",
    "        #ax.annotate(\"1\",xy=(o_fe_sample_1_array_mean[xi+offset], o_fe_sample_1_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"2\",xy=(o_fe_sample_2_array_mean[xi+offset], o_fe_sample_2_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"3\",xy=(o_fe_sample_3_array_mean[xi+offset], o_fe_sample_3_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"4\",xy=(o_fe_sample_4_array_mean[xi+offset], o_fe_sample_4_array_mean[yi+offset]), fontsize=12)\n",
    "        ax.annotate(\"5\",xy=(o_fe_sample_5_array_mean[xi+offset], o_fe_sample_5_array_mean[yi+offset]), fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz\n",
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "batch_size = 1\n",
    "input_rows = 1\n",
    "\n",
    "dummy_sample_1 = fetch_data_single(df = df, star_spectra = star_spectra, teff_min = 4800, \n",
    "teff_max = 4900, logg_min = 3.2, logg_max = 3.3, snr_min = 150, snr_max = math.inf)\n",
    "\n",
    "\n",
    "for batch_idx, (x) in enumerate(tqdm(dummy_sample_1)):\n",
    "    x = x.view(batch_size, x_dim)\n",
    "    x = x.to(DEVICE)    \n",
    "    x_hat, mean, log_var, z = model(x)\n",
    "\n",
    "\n",
    "make_dot(x_hat, params=dict(model.named_parameters()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13427881f06f13311079f5221e5dd632fdf9146891f6da22d47a93dcb9272d3a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
